## 本地搭建知识库
- https://juejin.cn/post/7468906236181839909?searchId=202502111043418A2C680815CA659B1C7C
- 本地访问地址 http://localhost
- ollama 服务 http://127.0.0.1:11434 http://localhost:11434
## DeepSeek-R1和DeepSeek-V3的区别
- 参考文章 https://cloud.tencent.com/developer/article/2493720
DeepSeek R1和DeepSeek V3的模型结构一致，参数量也一致，R1是基于V3做强化学习得来的。R1主要的创新点都用在训练过程，推理过程和V3是一样的。

DeepSeek-R1-Zero 和 DeepSeek-R1 基于 DeepSeek-V3-Base 进行训练。有关模型架构的更多详细信息，请参阅 DeepSeek-V3 存储库。

## DeepSeek OpenAI Claude 价格比对
https://mofcloud.cn/blog/blog/2025-01-02-deepseek-v3/


## 如何给DeepSeek 投喂数据（知识库类型）
- https://blog.csdn.net/xiangzhihong8/article/details/145510676

## 微调DeepSeek 
- https://www.cnblogs.com/shanren/p/18707513


## 文档deepseek 为什么是一个重要的突破
- https://mofcloud.cn/blog/blog/2025-01-24-deepseek-key-points/


## 训练芯片和推理芯片
```
训练芯片的主要要求:

强大的计算能力 - 需要处理海量的矩阵运算,要求芯片有很高的算力密度和FLOPS。为了支持大规模参数更新,训练芯片通常采用FP32或BF16等高精度的浮点格式。
大容量显存 - 训练过程需要存储模型参数、梯度、优化器状态等数据,对显存容量要求很高。目前主流的训练芯片如NVIDIA A100/H100都配备数十GB甚至上百GB的HBM显存。
高带宽互联 - 分布式训练要求芯片之间能够快速交换梯度信息,需要NVLink、InfiniBand等高速互联技术支持。

推理芯片的主要要求:

低延迟 - 在线服务场景要求快速响应用户请求,芯片需要优化推理延迟。可以采用INT8等低精度格式来加速计算。
高吞吐 - 需要支持高并发的推理请求,要求芯片具备强大的处理能力。但对单精度浮点性能要求不如训练阶段高。
功耗效率 - 推理服务器长期运行,需要考虑功耗和散热问题。相比训练芯片,推理芯片更注重性能功耗比。
成本效益 - 由于部署规模较大,推理芯片需要在性能和成本之间取得平衡。可以采用更经济的芯片方案,如消费级GPU或专用推理加速器。

这些不同的需求导致训练和推理通常使用不同类型的芯片:训练倾向于使用高端数据中心GPU,而推理则可以采用更多样化的硬件方案,包括低端GPU、FPGA、ASIC等。
```


## H800和A800
- DeepSeek-V3完整训练一次需要2.788M H800 GPU小时


## Embeddings 向量处理
- RAG 系统高效检索提升秘籍：如何精准选择 BGE 智源、GTE 阿里与 Jina 等的嵌入与精排模型的完美搭配
  - https://juejin.cn/post/7438079312542777398?searchId=202502111119068DB8018869AF4D66A8B4
- https://github.com/FlagOpen/FlagEmbedding
- https://huggingface.co/BAAI/bge-m3

## deepseek openai claude 价格
- https://platform.openai.com/docs/pricing
- https://api-docs.deepseek.com/zh-cn/quick_start/pricing
- https://www.anthropic.com/pricing#anthropic-api

## 账号
- claude aehyok.0624@gmail.com
- openai aehyok@outlook.com

## sql语言模型
- https://youtu.be/MpTxJLcViuU

## 天翼云微调deepseek 大模型
- https://www.ctyun.cn/document/10026730/10943516
- llama-factory 微调示例 https://blog.csdn.net/David_house/article/details/139426591
- 安装CUDA https://blog.chintsan.com/archives/309
- 微调R1大模型 https://www.cnblogs.com/REN-Murphy/p/18711299 
## cursor中设置 ds
- https://www.cnblogs.com/ryanyangcs/p/18658025


## window 安装
安装完驱动程序 Studio 后，打开命令提示符，输入以下命令以验证安装：

NVIDIA控制面板=> 帮助=> 系统信息 => 组件 -> NVCUDA64
```
nvidia-smi
```

CUDA版本要尽量小于驱动版本

```
nvcc -V  查看CUDA版本
```


## llama_factory 导出模型
- https://blog.csdn.net/weixin_53162188/article/details/137754362

## 卸载显卡驱动
- ddu https://www.wagnardsoft.com/
- https://www.wagnardsoft.com/forums/viewtopic.php?t=5192





## 操作1——初始化git-lfs命令
```
curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash
sudo apt-get install git-lfs
git lfs install

```


## 操作2——下载github仓库
```
// 下载加速---https://www.autodl.com/docs/network_turbo/
source /etc/network_turbo

// 下载
git clone https://github.com/hiyouga/LLaMA-Factory.git
```


## 操作3--下载模型比如deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
```
git clone https://www.modelscope.cn/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B.git
```

## 操作4--创建python运行环境
```
// 创建环境
conda create -n llama_factory python=3.10
conda activate llama_factory

// 安装依赖
cd LLaMA-Factory
pip install -e .[torch,metrics]

// 运行环境
export CUDA_VISIBLE_DEVICES=0
python src/webui.py

// 出现如下运行成功
Running on local URL:  http://0.0.0.0:7860

// 通过audodl SSH 隧道工具进行端口映射
ssh指令 复制
ssh密码 复制
代理到本地端口： 将autodl服务器上端口（sshport） 复制到这里
代理到远程端口： 暂时没使用，默认即可

// 然后浏览器打开地址
http://127.0.0.1:sshport

//删除当前文件夹下的文件和文件夹
rm -rf \*

```

## 浏览器运行

## 新的GPU租用平台
- https://cloud.luchentech.com/


## 微调大模型留一个方案
- https://github.com/modelscope/ms-swift/blob/main/README_CN.md