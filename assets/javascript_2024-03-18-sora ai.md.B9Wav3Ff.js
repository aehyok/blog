import{_ as e,c as o,o as p,ag as a}from"./chunks/framework.Dv47j_Ni.js";const T=JSON.parse('{"title":"","description":"","frontmatter":{},"headers":[],"relativePath":"javascript/2024-03-18-sora ai.md","filePath":"javascript/2024-03-18-sora ai.md","lastUpdated":1749113618000}'),n={name:"javascript/2024-03-18-sora ai.md"};function i(s,r,t,f,A,S){return p(),o("div",null,r[0]||(r[0]=[a("<p>SORA:OpenAI的人工智能AI生成式视频大模型在2024年2月15号一经发布就引发了全球关注。硅谷AI视频论文作者这样评价道:&quot;相当好,这是毋庸置疑的No.1。&quot;那么SORA好在哪里?生成式AI视频的发展挑战在哪里?OpenAI的视频模型一定是正确的路线吗?所谓的世界模型达成共识了吗?</p><p>这期视频我们就通过与硅谷一线AI从业者的采访深度来聊聊生成式AI视频大模型的不同派系、发展史、大家的中意还有未来路线。</p><p>AI生成视频这个题我们其实在去年就想做了,因为当时跟很多人聊天,包括跟VC的投资人聊的时候,发现其实大家对于AI视频模型和XGBT这种大语言模型的区别并不是非常的清楚。但是为什么没做呢?就是因为在去年年底市场中做的最好的也就是比如说Runway这家公司旗下的Gen1和Gen2这两种的视频生成视频以及文字生成视频的功能,但我们生成出来的效果呢确实有点眼难尽。</p><p>比如说我们用Runway生成的一个视频Prompt提示词是&quot;Super Mario walking in a desert&quot;超级玛利奥漫步于沙漠中,结果出来的视频是这样的,怎么看怎么像玛利奥?是跳跃在月球上,无论是重力还是摩擦力,物理学在这段视频里面好像突然不复存在了。然后呢,我们就尝试了另外的一个提示词&quot;A group of people walking down a street at night with umbrellas on the windows of stores&quot;雨夜大街上一群人走在商铺窗户沿的伞下,那这段提示词呢也是一个投资人Garriel Harrison尝试过的,结果呢出来的视频是这样的,你看这空中漂浮的雨伞是不是很诡异?但是呢,这已经是去年代表着最领先技术的runway了。</p><p>那么之后呢,我们知道华人创始人Demiguo Changli的peak collapse也是火了一阵,被认为呢比runway效果呢稍好一些,但是依然受制于三到四秒的长度显示,并且呢生成的视频呢依然也存在着视频里面的理解逻辑、手部构图等等的缺陷问题。所以呢,在OpenAI发布Sora模型之前,甚至是AI视频模型呢并没有像ChargeGT还有MeetJourney这样的聊天还有文生图应用一样引发全球关注。其中很大原因啊就是因为生成视频的技术难度非常高。</p><p>那视频呢是二维空间加时间,从静态到动态,从平面到不同时间片段下面的平面显示出来的立体效果,不但需要强大的算力和算法,还需要解决一致性、连贯性、物理合理性还有逻辑合理性等等一系列的复杂问题。所以呢,生成视频大模型这个选题呢一直都在我们硅谷101的选题清单上,但是一直拖着没错,就像等生成视频AI视频模型有一个重大突破的时候我们再来做这个选题,结果没想到这么快这个时刻就来了。</p><p>这是新的工具,几个小时前被OpenAI称为Sora,这真的是疯狂!这真的像是AI的另一个ChargeGT Dolly的一段时间。Sora的展示毫无疑问是一个很重要的时刻,Sora的展示毫无疑问是一个很重要的时刻,Sora的展示毫无疑问是一个很重要的时刻,Sora的展示毫无疑问是一个很重要的时刻。</p><p>是吊打此前的Runway和Pika Labs的。那么首先最大的突破之一很直观的就是生成视频长度是大大的延长了。之前Runway和Pika都只能生成出3到4秒的视频,太短了。所以之前能够出圈的,我们看到的AI视频作品就只有一些快节奏的电影预告片,因为其他需要长一些素材的用途根本就无法被满足。而在Runway和Pika上如果需要更长的视频就需要自己不断提示叠加视频,但是我们视频后期剪辑师Jacob就发现就会出现一个大问题痛点,就是你在不断往后延长的时候,它后面的视频会出现变形,那就会导致你前后视频的画面不一致了,这张素材就用不了了。而Sora最新展示的论文和demo中就表示可以根据提示词直接生成一分钟左右的视频场景,与此同时Sora会兼顾视频中人物场景的变换以及主题的一致性,那么这样我们的剪辑会更加有效,剪辑师看了之后也直呼兴奋。它其中有一个视频是一个女孩走在东京的街头的那个视频,那个是真的让我觉得直接秒了之前全部的视频的AI模型了,因为它一边走你会发现它背后的那些广告牌上面的信息是不怎么变化的,其他的一些视频生成一个天空大自然这样的一些场景,它都未必能够保持到你视频第一秒跟最后一秒有一个一致性,在里面这次的Sora它是全程广告牌这么细节的一些画面都能够保持着连贯性。所以就算在视频动态的运动情况下,随着空间的移动和旋转,Sora视频中出现的人物和物体也会保持场景一致性的移动。</p><p>那么第三,Sora可以接受视频图像或者提示词作为输入,模型会根据用户的输入去生成视频。那比如说公布出来的demo中一朵爆开的云,那么这意味着Sora模型可以基于静态图像来制作动画,做到时间上向前或者向后来扩展视频。</p><p>第四,Sora可以读取不同无论是宽屏还是垂直的视频进行采样,也可以根据同一个视频去输出不同尺寸的视频并且保持风格稳定。那比如说这个小海龟的样片,那其实对我们视频后期的帮助是非常大的。那么现在YouTube还有B站等等1920x1080p横屏的视频,我们需要重新的去剪辑成垂直的1080p1080x1920的这个视频来适配抖音还有TikTok等短视频平台,但可以想象的之后也许就可以通过Sora一键EI转换,那这也是我非常期待的功能了。</p><p>第五,远距离相关性和时间连贯性更强了。那么此前AI生成的视频有个很大的困难就是时间的连贯性,但是Sora能够很好的记住视频中的人和物体,即使被暂时挡住或者移出画面之后再出现的时候也能够按照物理逻辑的让视频保持连贯性。那比如说Sora公布出来的视频中的视频中的人和物体这个小狗的视频啊,当人们走过它画面被完全挡住再出现它的时候呢,它也能够自然的继续运动保持时间和物体的连贯性。</p><p>那么第六,Sora模型已经可以简单的模拟世界整态的动作了。那比如说画家在画布上留下新的笔触,那么这些笔触呢会随着时间的退移而持续的存在。或者呢,一个人他吃汉堡的时候呢会留下汉堡上面的咬分。那么有比较乐观的解读就认为啊,这意味着模型具备了一定的通常性能够实能力,能够理解运动中的物理世界,也能够预测到画面的下一步会发生什么。因此呢,以上的这几点,Sora模型带来的震撼更新啊是极大的提高了外界对于生成式AI视频发展的期待还有兴奋值啊。</p><p>虽然Sora也会出现一些逻辑错误呢,比如说小猫出现了三只爪子,街景中有不符合常规的障碍物,人在跑步机上的方向反了等等。但是显然,比起之前的生成视频,无论是Runway、Pika、Stable Video等等基于Diffusion模型所呈现出来的视频模型,Sora都是绝对的顶线者。而更重要的是啊,OpenAI似乎通过Sora想要证明,对算理对参数的大力出奇迹的方式也可以适用到生成式视频上面来,并且呢通过扩散模型和代言模型的整合,那么这样的模型新路线来形成所谓的世界模型的基础。而这些观点呢也是在AI界引发了极大的争议和讨论。</p><p>那么接下来我们就来试图回顾一下生成式AI大模型的技术发展之路以及试图解析一下Sora的模型是怎么运作的,它到底是不是所谓的世界模型呢?好嗯,大家做好小本本拿出来,接下来我们就开始讲模型了。</p><p>AI生成视频的早期阶段主要依赖于GAN生成式对抗网络和VAE变分自编码器这两种模型,但是啊,这两种方法生成的视频内容相对属性相对的单一和静态,而且分辨率往往不太好,完全的没有办法去进行商用,所以这两种模型啊,我们就先不讲啊。</p><p>之后呢,AI生成视频就演变成了两种技术路线,一种的是专门用于视频领域的扩散模型,一种呢则是Transformer模型。我们先来说一下扩散模型的路线,那么跑出来的公司就有Runway还有Pika Labs等等。的扩散模型的优冷是Diffusion Model,很多人不知道如今最重要的开源模型Stable Diffusion的原始模型呢,就是有Runway和慕尼黑大学团队一起发布的,而Stable Diffusion它本身呢也是Runway的核心产品视觉编辑器Gen1和Gen2背后的底层技术基础。</p><p>Runway的Gen1模型在2023年2月发布,允许大家通过文本或者图像改变原视频的视觉风格,例如将手机拍摄的现实街景变成赛博视界。而在6月份,Runway是发布了Gen2,进一步的能够将用户输入的文本提示词直接生成为视频。</p><p>那扩散模型的原理大家一听这个名字扩散模型就能够稍微的get到是通过逐渐扩散来生成图像或者视频,但为了更好的给大家解释模型的原理,我们也是邀请到了之前Meta的Maker Video模型的论文作者之一,目前在亚马逊ACI团队从事视频生成模型的张颂阳博士来给我们做一个解释。</p><p>另外,好,我们还 repository其实设计对我们逐渐樕 makers这个可能会很难它分成了很多步。比如说我分成一千步,比如说我加一点点噪声,它能够换原它的去噪声出来什么样子,然后噪声加得比较多的时候,我该怎么去用一个模型怎么去预测噪声,然后逐渐地去把这噪声慢慢地去掉。你比如说原来是一个水跟墨已经完全混合在一起了,你想办法就是怎么去预测它一步一步它如何再变回之前的那一滴墨水的样子,它是一个扩散的一个逆过程。</p><p>张颂阳博士解释的很形象,扩散模型的核心思想是通过不断地向原始噪声引入随机性,逐步生成逼真的图像或者视频。而在这个过程分为了四步:</p><p>第一是初始化,扩散模型开始于一个随机的噪声图像或者视频帧作为初始的输入。</p><p>第二就是扩散过程,也被称为前向过程forward process,扩散过程的目标是让图片变得不清晰,最后变成完全的噪声。</p><p>第三步叫反向过程reverse process,又被称为 backward diffusion。那么这个时候我们就会引入神经网络了,比如说基于卷积神经网络CNN的unit结构,在每个时间步预测要达到现在这一帧模糊的图像所添加的噪声,从而通过去除这种噪声来生成下一帧的图像,以此来形成图像的逼真内容。</p><p>第四步就是重复步骤,直到达到所需要的生成图像或者视频的长度。</p><p>以上就是video to video或者picture to video的生成方式,也是runway的gen1的大概的顶层技术运行的方式。但是如果要达到输入提示词来达到text to video,那么就要多加几个步骤了。</p><p>比如说我们拿谷歌在2022年周旬发布的imagine模型来说明一下举个例子,我们的提示词是&quot;a boy is riding on the rocket&quot;提着火箭的男孩。那么这段提示词会被转换为token标记,并且传递给imagine模型的编码器text encoder。谷歌imagine模型接着会用t5 xxl llm编码器将输入文本编码为嵌入embeddings。那么这些嵌入代表着我们的文本提示词,但是呢以机器可以理解的方式进行编码。</p><p>之后呢,这些嵌入文本会被传递给一个图像生成器image generator。那么这个图像生成器的会生成64x64分辨率的低分辨率图像。那么之后呢,imagine模型会被传递给一个图像生成器image generator,那么这个图像生成器image generator那会利用超分辨率扩散模型将图像从64x64升级到256x256,然后呢再加一层超分辨率扩散模型,最后生成与我们的文本提示词紧密结合的1024x1024的高质量图像。</p><p>那么简单总结来说,在这个过程当中,扩散模型从随机造生图像开始,在去造过程中使用编码文本来生成高质量的图像。那么问题来了,为什么生成视频要比生成图片困难这么多呢?它的原理实际上还是一样的,只不过唯一一个区别就是多了一个时间轴。就是刚刚我们说的图片它是一个2D的,它是高度跟宽度。视频它多一个时间轴,它就是一个3D的,它就是高度宽度还有一个时间。</p><p>然后它在做这个学习这个扩散的逆过程的过程当中呢,就是相当于以前的是一个2D的一个逆过程,现在变成一个3D的逆过程,就是这么一个区别。所以说图片上的存在的问题,比如说你像这些一个生成的人脸它是不是真实啊,那我们如果图片存在这样的问题,我们视频也一样会存在这样的问题。</p><p>对于视频来说,它有些它有些独特的一些问题,就比如说那个画面它那个主体它是不是保持一致的。我觉得目前对于像风景这样的其实效果都还可以,然后但是如果涉及到人的话,因为人的这些要求可能会更精细,人的这个难度会更高。然后还有一个目前的一个难点,我觉得也在努力的一方向就是怎么把视频变得更长,因为目前来说的话,只生成两秒三秒四秒这样的视频,其实远远版出不了现在的另一场景。</p><p>扩散模型比起之前的GAN的模型来说有三个主要的优点。那第一呢就是稳定性,训练过程通常更加的稳定,不容易的陷入模式崩溃或者模式塌陷的问题。那么第二就是生成图像质量,扩散模型可以生成高质量的图像或者视频,尤其在训练充分的情况下,生成的结果通常比较的逼真。第三呢就是无需特定的架构,扩散模型呢不依赖于特定的网络结构,兼容性好,但很多不同类型的神经网络都可以拿来用。</p><p>然而呢,扩散模型也有两大主要的缺点。那包括首先啊,训练成本高。与一些其他生成模型相比,扩散模型的训练可能会比较的昂贵,因为呢它需要在不同造生程度的情况下学习去造,需要训练的时间更久。其次啊,生成花费的时间更多,因为生成的时候需要逐步的去造,生成图像或者视频而不是一次性的生成整个样本。</p><p>我们其实现在木画生成长的视频一个很重要原因就是我们的显存是有限的,你生成一张图片是可能占了一部分的显存,然后你如果生成六张图片它就占了可能差不多就把这些给占满了,当你需要生成更多张图片的时候你就得想办法怎么去继续考虑之前已经生成的这些信息,然后再去预测后面该生成什么样的信息。就是它是在首先在模型上面就提了一个更高的要求,当然算力上面也是一个问题。就是或许过很多年之后我们的显存会非常的大,可能我们也就不存在这样的问题了,也是有可能的。但是就目前来说,当下我们是需要一个更好的一个算法。</p><p>但是可能这个问题如果有更好的硬件可能这个问题就不存在。所以这注定了目前的视频扩散模型本身可能不是最好的算法。虽然Runway和PK Labs等代表的公司一直在优化其算法。</p><p>那么我们接下来来聊聊另外一个派别,基于Transformer架构的大语言模型生成视频技术入线。谷歌是一个传统的传统模型设计计划,它是在2023年12月底发布了基于大语言模型的生成式AI视频模型VideoPoet。那么这在当时被视为是生成视频领域当中扩散模型之外的另外一种解法和出路。</p><p>那么它是怎么个原理呢?那么大语言模型生成视频是通过理解视频内容的时间和空间关系来实现的。谷歌的VideoPoet是一个利用大语言模型来生成视频的例子。那么这个时候让我们再次的请出生成式AI科学家张颂阳博士来给我们做一个评论,做一个生动的解释。</p><p>那么简单来说,基于大语言模型的VideoPoet是这样运作的。那么第一,输入和理解,首先VideoPoet接收文本、声音、骨片、深度图、光流图或者有带编辑的视频作为输入。</p><p>那么第二就是视频和声音的编码,因为文本天然就是离散的形式,大语言模型自然而然就要求输入和输出必须是离散的特征。然而视频和声音是连续量。那么为了让大语言模型也能够让图片和视频或者声音作为输入和输出,那么这就是输入和输出的特征。然后这里VideoPoet将视频和声音编码成离散的Token。在深度学习当中Token是一个非常重要的概念,它是指一组符号或者标识符用于表示一组数据或者信息中的一个特定元素。那么在VideoPoet的例子当中通俗一点可以理解成视频的单词和声音的单词。</p><p>第三,模型训练和内容生成。那么有了这些Token词汇就可以根据用户给的输入,像学习文本Token那样训练一个Transformer去训练文本,去学习整个预测视频的Token。那模型就会开始生成内容,对于视频生成这意味着模型需要创建连贯的帧序列,这些帧不仅在视觉上符合逻辑还要在时间上保持连贯性。</p><p>那第四就是优化和微调了,生成的视频可能需要进步的优化和微调以确保质量和连贯性,这可能包括调整颜色光照还有帧之间的过渡等等。VideoPoet利用深度学习技术来优化生成的视频来确保它们既符合文本的描述又能够在视觉上吸引人。</p><p>那第五就是输出,那最后生成的视频会被输出供最终的用户来观看。</p><p>但是单元模型生成视频的路线也是优点和缺点并存的。首先来说说优点,那么第一就是高度理解能力。基于Transformer架构的单元模型能够处理和理解大量的数据,包括复杂的文本还有图像信息。那么这使得模型能够具有跨模态的理解和生成能力,能够很好的学习提到文本和图片视频不同模态之间的一个关联的能力。那么这使得它们在将文本描述转换成视频内容的时候能够生成更准确和相关的输出。</p><p>那么第二就是处理长序列数据,由于自注意力机制,Transformer模型特别擅长处理这种长序列的数据。那么这对于视频生成尤为重要,因为视频它本质上就是长序列的一个视觉的表示。</p><p>第三就是Transformer的它是一个模型的可扩展性。通常来说模型越大拟合的能力就越强,但是当模型大到一定程度的时候,卷积神经网络性能受模型增大带来的增益就会放缓,甚至停止。但是Transformer依然能够持续的增长,Transformer在大语言模型当中已经证明了这一点,如今在图片和视频生成这一领域也慢慢的被验证了。</p><p>那么再来说一下缺点。第一就是资源密集型,用大语言模型生成视频质量的视频需要大量的计算资源,因为呢用单元模型的录像是将视频编码成token,往往会比一句话或者一段话的词汇量要大得多得多,同时如果一个一个的去预测会让时间的开销非常之大。也就是说这可能使得Transformer模型的训练和推理过程会变得非常的昂贵和时间消耗。</p><p>但是有一个问题我觉得挺本质的就是Transformer它不够快,这个是很本质的问题。因为Transformer它一个小时的训练过程是很短的一个小方块一个小方块预测,我扩散模型我直接一张图就出来了。所以说它会肯定Transformer肯定会比较慢的,它慢了有一个具象的这样的一个数据吗?就是能慢多少就比如说我一张图我直接出一张图,那就是比如说我用四步就是四步去生成出来就是等于是4。现在目前做的好的话四步我看做的效果还是不错的。然后但是你要是用Transformer的话,比如说你画16x16的方格,那就是16x16,那就等于256了。4是相当于我去做去造的迭代的四次,然后Transformer的话它是相当于我去预测一张图片比如说16x16的话我就预测256个词,它们的量纲肯定不一样,但是它们的复杂度你是可以看出来的。底部性模型它的复杂度是一个长数题,但是Transformer的复杂度它实际上是一个一个宽度乘高度。所以说从此来六角度来说,肯定是复杂模型会更优一些。我觉得这东西可能你如果是图片越大的话字面率高的话,Transformer的问题可能会越大。</p><p>Transformer模型的另外一些问题还包括质量波动,尽管Transformer模型能够生成创造性的视频内容,但是输出的质量可能不稳定,特别是对于复杂的未经过充分训练的模型。</p><p>那么第三点是数据的依赖性,Transformer模型的性能在很大程度上取决于训练数据的质量还有多样性。如果训练数据有限或者有偏差,那么生成的视频可能就无法准确的反映输入的意图或者在多样性上存在限制。</p><p>那么第四就是理解和逻辑限制了。那么虽然Transformer模型在理解文本和图像内容方面是取得了进步,但是他们可能依然难以完全的把握复杂的人类的情感、幽默或者细微的社会文化信号。那么这可能会影响生成视频的相关性还有吸引力。</p><p>那么第五就是轮理和偏见问题了。AI视频生成技术可能会无意中复制或者放大训练数据中的偏见,导致轮理问题。</p><p>不过说到第五点突然想起来最近有这么一个新闻,有说谷歌的多模态大模型Gemini当中无论你输入什么,人出来的都是有色人种,包括美国的开国元勋还出现了黑人女性版的教皇,维京人也是有色人种,生成的Elon Musk也是黑人。这背后的原因很可能也是谷歌为了更正Transformer架构中的偏见给加入了AI道德和安全方面的调整指令,结果调过头了,就出了这么一个大乌龙。但好巧不巧这个事情发生在了OpenAI发布Sora之后,确实又让谷歌被群嘲了一番。</p><p>不过业内人士也指出,以上的这五点问题也不是Transformer架构所独有的,目前任何的生成模型都有可能存在这些问题,只是不同模型在不同的方向的优劣是稍有不同。</p><p>所以我们到这里总结一下,扩散模型和Transformer模型这支视频都有不胜令人满意的地方。那么身为技术最为前沿的公司OpenAI他们是怎么解决的呢?也许你已经猜到了,这两个模型各有千秋,那我们就把它们结合在一起,会不会一加一大于二的于是Sora也就是扩散模型和Transformer模型的结合。说实话,目前外界对Sora的细节还是未知的,现在也没有对公众开放,连waiting list都没有开放,只邀请了业界和设计界极少数的人来使用,那么产出的视频也都在网上公开了。但对于技术更多是基于OpenAI给出的效果视频的一个猜测和分析。</p><p>OpenAI给出了一个比较模糊的技术解释,Sora是一个视频生成模型,这意味着它看了很多视频资料和学习生成真实视频。它做的正确的方式是这样的,它其实是从输出的模型例如DALI和LLM例如GPT的系列,它在之间的设计,它像DALI一样,但是在建筑上看起来像GPT的系列,但在高层的设计上它只是为了制造真实世界和数码世界的视频以及所有的内容,但是中间有很多的技术细节是缺失的。</p><p>但是我们先从Sora公开的这篇技术解析来看看OpenAI的扩散加代言模型的效果,来看看OpenAI的扩散加代言模型的效果,来看看OpenAI的扩散加代言模型的效果和技术路线是如何操作的。</p><p>Sora在开头就说得很清楚,OpenAI在可变持续时间、分辨率和宽高比的视频和图像上联合训练文本条件扩散模型Text Conditional Diffusion Models,同时利用对视频和图像潜在代码的时空补丁SpaceTime Patches进行操作的Transformer架构。所以Sora模型的生成步骤可能包括：</p><p>第一步,视频压缩网络。在基于代言模型的视频生成技术中,我们提到过视频编码成一个一个离散的Token。那么这里Sora也是采用了相同的想法,视频是一个三维的输入,也就是二维空间加一维的时间。这里将视频在三维空间中均分成一个一个小的Token,被OpenAI称为时空补丁SpaceTime Patches。那么之后还需要用到一个训练好的Encoder和Decoder将视频数据进一步的压缩到影空间Latent Space里面去进行训练。那么这里也会用到一个影空间扩散模型Latent Diffusion Model。</p><p>第二步就是文本理解了。因为Sora由OpenAI纹身图模型Dali3的加持,可以将许多没有文本标注的视频自动进行标注,并且就于视频生成的训练。同时因为有GBT的加持,可以将用户的输入扩写成更加详细的描述,使得生成的视频获得更加贴合用户的输入。并且Transformer架构能够帮助Sora模型更有效的学习和提取特征,获取和运输的设计和理解大量的细节信息,增强模型对被建过数据的泛化能力。</p><p>比如说你输入一个卡通弹鼠在跳Disco,那么GBT就会帮助联想说你得戴个低听把、戴个墨镜把、穿个花衬衫把、灯光要闪耀吧,那背后还有一堆各种各样的动物在一起蹦打吧等等等等,来发挥联想能力解释输入的Probe。所以GBT能够展开的解释和细节、风格和交付程度将会决定Sora生成的有多好。而GBT模型就是OpenAI它自家的,不像其他AI视频startup公司还需要去调用GBT模型,那么OpenAI给Sora的GBT架构的调取效率还有它的深广度肯定都是最好最高的。那么这可能也是为什么Sora会在语音理解上做得更好。</p><p>那么第三步就是 Diffusion Transformer 成像了。Sora是采用Diffusion和Transformer结合的方式。之前我们在基于单元模型的视频生成技术当中就提到过,Transformer具有较好的可拓展性,意思就是说Transformer的结构它会随着模型的增大效果会越来越好。那么这一特征并不是所有模型都具备的,比如说模型大到一定程度的时候我们刚才也提到过卷积神经网络性能就会受到模型增大带来的增益放缓,甚至停止。而Transformer依然是能够持续增长的。</p><p>很多人会注意到,Sora在保持画面物体的稳定性、一致性还有画面旋转等等都表现出稳定的能力,是远超之前的Runway、Pika、Stable Video等等基于Diffusion模型所呈现出来的视频模型。还记得我们在说扩散模型的时候也说到视频生成的挑战在于生成物体的稳定性还有一致性。那么这是因为虽然Diffusion是视频生成技术的主流,但是之前的工作一直局限在基于卷积神经网络的结构,并没有发挥出Diffusion的取决性。而Sora可能是很巧妙的结合了Diffusion和Transformer这两者的优势,让视频生成技术获得了更大的一个提升。</p><p>而更深一步的说,Sora生成的视频连续性可能是通过Transformer Self-Attention自注意力机制获得的。Sora可以将时间离散化,然后通过自注意力机制理解前后时间线的关系。而自注意力机制的原理就是每个时间点和其他所有时间点去产生联系,是Diffusion Model所不具备的。</p><p>那目前外界有一些观点猜测,在我们之前说到了这个扩散模型的第三步骤当中,Sora选择了将Unit架构替换成了Transformer的架构。这让Diffusion扩散模型作为一个画师开始逆扩散画画的时候,在消除噪音的过程当中能够根据关键词特征值对应的可能性概率,在OpenAI海量的数据库中找到更贴切的部分来进行下笔。</p><p>那么在采访另外一位AI从业者的时候,她用了一个非常深的例子来解释这里的区别。她说扩散模型预测的是噪音,从某个时间点的画面减去预测的噪音得到的就是最原始的没有噪音的画面,也就是最终生成的画面。那么这里更像是雕塑,就像米开朗基罗说的,他指示的遵照上帝的旨意将实料上不应该存在的部分去掉,最终他才从中创造出伟大的雕塑作品。而Transformer则是通过自注意力机制理解时间线之间的关联,让这尊雕塑从实作上走了下来,是不是还挺形象的。</p><p>最后,Sora的Transformer加Diffusion Model将时空补丁生成图片,然后图片再拼接为视频序列。那么一段Sora的视频就生成了。</p><p>那么说实话,Transformer加扩散模型的方法论并不是OpenAI独创的。在OpenAI发布Sora之前,我们在和张颂阳博士在今年1月份进行采访的时候,他就已经提到说Transformer加扩散模型的方式已经在行业当中开始普遍的被研究了。</p><p>目前因为我们看到一些把Transformer的模型做到多根Diffusion结合,然后效果可能也不差,甚至可能都没有先说的可能会更好。所以说这个东西我不确定以后模型会怎么发展,我觉得可能是两者结合的一种。就是Transformer比如说它预测下一个视频对天然的优势就是它可以预测电场的一些东西,Diffusion它的虽然质量高但是Diffusion它目前很多做法它是生成固定帧数的,怎么把两个东西结合在一起是一个我们会研究的一个过程。</p><p>所以这也解释了为什么OpenAI现在要发布SORA,其实在OpenAI的论坛上官方也澄清说,SORA现在并不是一个成熟的产品。所以它不是已发布的产品,也不公开,也没有等候名单,没有Waiting List,也没有预计的发布日期。外界也有分析认为,SORA还不成熟,OpenAI的算力也不一定能够承受SORA被公开,同时还有公开之后的假新闻、安全还有道具道德等等问题,所以SORA不一定会很快的正式发布。</p><p>但是因为Transformer加扩散模型Diffusion已经成为了业内普遍尝试的方向,那么这个时候OpenAI需要展示出SORA的能力来,在目前竞争日益白热化的生成式AI视频领域中去重申自己行业的顶线地位。</p><p>而有了OpenAI的验证之后,我们基本可以确定的是AI视频生成方向会转变到新的技术结合。而OpenAI在发表的技术文章当中也明确的指出,在XGBT上的巨量参数大力出奇迹的方式被证明在AI视频生成上。</p><p>OpenAI在文章当中说我们发现视频模型在大规模训练时表现出许多有趣的涌现功能,这些功能使SORA能够模拟现实世界中人、动物和环境的某些方面。那么这说明SORA和GBT-3的时候一样出现了涌现Emergence。而这意味着与GBT大语言模型一样,AI视频也需要更多的参数、更多的GPU算力、更多的资金投入。Scalning Law依然是目前生成式AI的绝招。</p><p>而这可能也意味着生成式AI视频也许最终也会成为大公司的游戏。因为我们现在视频的参数量只是在B点级,但是像图片里面他们之前的Stable Diffusion模型,他们后来出了一个Stable Diffusion XL,他们也是把模型做大了,然后也带来了一些比较好的一个效果。然后也不是说比较好的效果就是他们能做更真实的那图片,然后效果也会更明显一些。我觉得这是一个趋势,就是未来肯定会把参数量做大的,但是说它带来的增益会有多少也取决于你目前的这个模型的结构以及你的数据量,你的数据是什么。</p><p>那么以上呢就是我们对SORA非常初步的分析。那么再次的说明一下,因为SORA非常多的技术细节没有公开,所以呢我们的很多分析呢也是从外部视角去做了一个猜测和解读啊。如果有不准确的地方也欢迎大家来纠错指正和探讨,因为视频长度的关系,我们讲业界对世界模型的争议和探讨放在了下期视频当中。其中呢非常简明易懂的阐述了Yan LeCun等AI领军人物对世界模型的思考,以及我们试图来回答一个问题,SORA到底是不是世界模型。大家记得关注我们硅谷101,不要错过下集的更新哦。</p><p>最后来打一个小广告,如果你想跟业界200多名顶尖的AI专家们有更多的面对面交流机会,来了2024年5月29号到31号在宙星山举行的GenAI Summit SF大会。OpenAI旗下的视频生成模型SORA团队核心成员、马斯克的XAI团队创始成员、还有包括OpenAI、Mistral AI、微软、英伟达、谷歌、DeepMind、Meta、AIPerplexity、Character.AI的核心成员和研究科学家们也将在现场分享他们的前沿发现和见解。</p><p>如果大家感兴趣可以访问GenAI Summit的网站或者在我们的评论区点击购票链接。这次大会也为硅谷101的观众们准备了一些小福利,使用优惠码SV10150购买大会三日门票可以享受50美元的购票优惠,单日门票也可以使用优惠码SV105或SV10120购票享受20美元的购票优惠。如果想来见见AI行业中的领军人物,这个大会会是一个很好的机会。那我们就下个视频再见啦。</p>",75)]))}const m=e(n,[["render",i]]);export{T as __pageData,m as default};
