import{_ as e,c as s,o as n,ag as l}from"./chunks/framework.mJdS8VI0.js";const u=JSON.parse('{"title":"","description":"","frontmatter":{},"headers":[],"relativePath":"article/code/2025-02-10-deepseek.md","filePath":"article/code/2025-02-10-deepseek.md","lastUpdated":1753779437000}'),t={name:"article/code/2025-02-10-deepseek.md"};function p(i,a,r,o,c,h){return n(),s("div",null,a[0]||(a[0]=[l(`<h2 id="本地搭建知识库" tabindex="-1">本地搭建知识库 <a class="header-anchor" href="#本地搭建知识库" aria-label="Permalink to &quot;本地搭建知识库&quot;">​</a></h2><ul><li><a href="https://juejin.cn/post/7468906236181839909?searchId=202502111043418A2C680815CA659B1C7C" target="_blank" rel="noreferrer">https://juejin.cn/post/7468906236181839909?searchId=202502111043418A2C680815CA659B1C7C</a></li><li>本地访问地址 <a href="http://localhost" target="_blank" rel="noreferrer">http://localhost</a></li><li>ollama 服务 <a href="http://127.0.0.1:11434" target="_blank" rel="noreferrer">http://127.0.0.1:11434</a> <a href="http://localhost:11434" target="_blank" rel="noreferrer">http://localhost:11434</a></li></ul><h2 id="deepseek-r1和deepseek-v3的区别" tabindex="-1">DeepSeek-R1和DeepSeek-V3的区别 <a class="header-anchor" href="#deepseek-r1和deepseek-v3的区别" aria-label="Permalink to &quot;DeepSeek-R1和DeepSeek-V3的区别&quot;">​</a></h2><ul><li>参考文章 <a href="https://cloud.tencent.com/developer/article/2493720" target="_blank" rel="noreferrer">https://cloud.tencent.com/developer/article/2493720</a> DeepSeek R1和DeepSeek V3的模型结构一致，参数量也一致，R1是基于V3做强化学习得来的。R1主要的创新点都用在训练过程，推理过程和V3是一样的。</li></ul><p>DeepSeek-R1-Zero 和 DeepSeek-R1 基于 DeepSeek-V3-Base 进行训练。有关模型架构的更多详细信息，请参阅 DeepSeek-V3 存储库。</p><h2 id="deepseek-openai-claude-价格比对" tabindex="-1">DeepSeek OpenAI Claude 价格比对 <a class="header-anchor" href="#deepseek-openai-claude-价格比对" aria-label="Permalink to &quot;DeepSeek OpenAI Claude 价格比对&quot;">​</a></h2><p><a href="https://mofcloud.cn/blog/blog/2025-01-02-deepseek-v3/" target="_blank" rel="noreferrer">https://mofcloud.cn/blog/blog/2025-01-02-deepseek-v3/</a></p><h2 id="如何给deepseek-投喂数据-知识库类型" tabindex="-1">如何给DeepSeek 投喂数据（知识库类型） <a class="header-anchor" href="#如何给deepseek-投喂数据-知识库类型" aria-label="Permalink to &quot;如何给DeepSeek 投喂数据（知识库类型）&quot;">​</a></h2><ul><li><a href="https://blog.csdn.net/xiangzhihong8/article/details/145510676" target="_blank" rel="noreferrer">https://blog.csdn.net/xiangzhihong8/article/details/145510676</a></li></ul><h2 id="微调deepseek" tabindex="-1">微调DeepSeek <a class="header-anchor" href="#微调deepseek" aria-label="Permalink to &quot;微调DeepSeek&quot;">​</a></h2><ul><li><a href="https://www.cnblogs.com/shanren/p/18707513" target="_blank" rel="noreferrer">https://www.cnblogs.com/shanren/p/18707513</a></li></ul><h2 id="文档deepseek-为什么是一个重要的突破" tabindex="-1">文档deepseek 为什么是一个重要的突破 <a class="header-anchor" href="#文档deepseek-为什么是一个重要的突破" aria-label="Permalink to &quot;文档deepseek 为什么是一个重要的突破&quot;">​</a></h2><ul><li><a href="https://mofcloud.cn/blog/blog/2025-01-24-deepseek-key-points/" target="_blank" rel="noreferrer">https://mofcloud.cn/blog/blog/2025-01-24-deepseek-key-points/</a></li></ul><h2 id="训练芯片和推理芯片" tabindex="-1">训练芯片和推理芯片 <a class="header-anchor" href="#训练芯片和推理芯片" aria-label="Permalink to &quot;训练芯片和推理芯片&quot;">​</a></h2><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>训练芯片的主要要求:</span></span>
<span class="line"><span></span></span>
<span class="line"><span>强大的计算能力 - 需要处理海量的矩阵运算,要求芯片有很高的算力密度和FLOPS。为了支持大规模参数更新,训练芯片通常采用FP32或BF16等高精度的浮点格式。</span></span>
<span class="line"><span>大容量显存 - 训练过程需要存储模型参数、梯度、优化器状态等数据,对显存容量要求很高。目前主流的训练芯片如NVIDIA A100/H100都配备数十GB甚至上百GB的HBM显存。</span></span>
<span class="line"><span>高带宽互联 - 分布式训练要求芯片之间能够快速交换梯度信息,需要NVLink、InfiniBand等高速互联技术支持。</span></span>
<span class="line"><span></span></span>
<span class="line"><span>推理芯片的主要要求:</span></span>
<span class="line"><span></span></span>
<span class="line"><span>低延迟 - 在线服务场景要求快速响应用户请求,芯片需要优化推理延迟。可以采用INT8等低精度格式来加速计算。</span></span>
<span class="line"><span>高吞吐 - 需要支持高并发的推理请求,要求芯片具备强大的处理能力。但对单精度浮点性能要求不如训练阶段高。</span></span>
<span class="line"><span>功耗效率 - 推理服务器长期运行,需要考虑功耗和散热问题。相比训练芯片,推理芯片更注重性能功耗比。</span></span>
<span class="line"><span>成本效益 - 由于部署规模较大,推理芯片需要在性能和成本之间取得平衡。可以采用更经济的芯片方案,如消费级GPU或专用推理加速器。</span></span>
<span class="line"><span></span></span>
<span class="line"><span>这些不同的需求导致训练和推理通常使用不同类型的芯片:训练倾向于使用高端数据中心GPU,而推理则可以采用更多样化的硬件方案,包括低端GPU、FPGA、ASIC等。</span></span></code></pre></div><h2 id="h800和a800" tabindex="-1">H800和A800 <a class="header-anchor" href="#h800和a800" aria-label="Permalink to &quot;H800和A800&quot;">​</a></h2><ul><li>DeepSeek-V3完整训练一次需要2.788M H800 GPU小时</li></ul><h2 id="embeddings-向量处理" tabindex="-1">Embeddings 向量处理 <a class="header-anchor" href="#embeddings-向量处理" aria-label="Permalink to &quot;Embeddings 向量处理&quot;">​</a></h2><ul><li>RAG 系统高效检索提升秘籍：如何精准选择 BGE 智源、GTE 阿里与 Jina 等的嵌入与精排模型的完美搭配 <ul><li><a href="https://juejin.cn/post/7438079312542777398?searchId=202502111119068DB8018869AF4D66A8B4" target="_blank" rel="noreferrer">https://juejin.cn/post/7438079312542777398?searchId=202502111119068DB8018869AF4D66A8B4</a></li></ul></li><li><a href="https://github.com/FlagOpen/FlagEmbedding" target="_blank" rel="noreferrer">https://github.com/FlagOpen/FlagEmbedding</a></li><li><a href="https://huggingface.co/BAAI/bge-m3" target="_blank" rel="noreferrer">https://huggingface.co/BAAI/bge-m3</a></li></ul><h2 id="deepseek-openai-claude-价格" tabindex="-1">deepseek openai claude 价格 <a class="header-anchor" href="#deepseek-openai-claude-价格" aria-label="Permalink to &quot;deepseek openai claude 价格&quot;">​</a></h2><ul><li><a href="https://platform.openai.com/docs/pricing" target="_blank" rel="noreferrer">https://platform.openai.com/docs/pricing</a></li><li><a href="https://api-docs.deepseek.com/zh-cn/quick_start/pricing" target="_blank" rel="noreferrer">https://api-docs.deepseek.com/zh-cn/quick_start/pricing</a></li><li><a href="https://www.anthropic.com/pricing#anthropic-api" target="_blank" rel="noreferrer">https://www.anthropic.com/pricing#anthropic-api</a></li></ul><h2 id="账号" tabindex="-1">账号 <a class="header-anchor" href="#账号" aria-label="Permalink to &quot;账号&quot;">​</a></h2><ul><li>claude <a href="mailto:aehyok.0624@gmail.com" target="_blank" rel="noreferrer">aehyok.0624@gmail.com</a></li><li>openai <a href="mailto:aehyok@outlook.com" target="_blank" rel="noreferrer">aehyok@outlook.com</a></li></ul><h2 id="sql语言模型" tabindex="-1">sql语言模型 <a class="header-anchor" href="#sql语言模型" aria-label="Permalink to &quot;sql语言模型&quot;">​</a></h2><ul><li><a href="https://youtu.be/MpTxJLcViuU" target="_blank" rel="noreferrer">https://youtu.be/MpTxJLcViuU</a></li></ul><h2 id="天翼云微调deepseek-大模型" tabindex="-1">天翼云微调deepseek 大模型 <a class="header-anchor" href="#天翼云微调deepseek-大模型" aria-label="Permalink to &quot;天翼云微调deepseek 大模型&quot;">​</a></h2><ul><li><a href="https://www.ctyun.cn/document/10026730/10943516" target="_blank" rel="noreferrer">https://www.ctyun.cn/document/10026730/10943516</a></li><li>llama-factory 微调示例 <a href="https://blog.csdn.net/David_house/article/details/139426591" target="_blank" rel="noreferrer">https://blog.csdn.net/David_house/article/details/139426591</a></li><li>安装CUDA <a href="https://blog.chintsan.com/archives/309" target="_blank" rel="noreferrer">https://blog.chintsan.com/archives/309</a></li><li>微调R1大模型 <a href="https://www.cnblogs.com/REN-Murphy/p/18711299" target="_blank" rel="noreferrer">https://www.cnblogs.com/REN-Murphy/p/18711299</a></li></ul><h2 id="cursor中设置-ds" tabindex="-1">cursor中设置 ds <a class="header-anchor" href="#cursor中设置-ds" aria-label="Permalink to &quot;cursor中设置 ds&quot;">​</a></h2><ul><li><a href="https://www.cnblogs.com/ryanyangcs/p/18658025" target="_blank" rel="noreferrer">https://www.cnblogs.com/ryanyangcs/p/18658025</a></li></ul><h2 id="window-安装" tabindex="-1">window 安装 <a class="header-anchor" href="#window-安装" aria-label="Permalink to &quot;window 安装&quot;">​</a></h2><p>安装完驱动程序 Studio 后，打开命令提示符，输入以下命令以验证安装：</p><p>NVIDIA控制面板=&gt; 帮助=&gt; 系统信息 =&gt; 组件 -&gt; NVCUDA64</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>nvidia-smi</span></span></code></pre></div><p>CUDA版本要尽量小于驱动版本</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>nvcc -V  查看CUDA版本</span></span></code></pre></div><h2 id="llama-factory-导出模型" tabindex="-1">llama_factory 导出模型 <a class="header-anchor" href="#llama-factory-导出模型" aria-label="Permalink to &quot;llama_factory 导出模型&quot;">​</a></h2><ul><li><a href="https://blog.csdn.net/weixin_53162188/article/details/137754362" target="_blank" rel="noreferrer">https://blog.csdn.net/weixin_53162188/article/details/137754362</a></li></ul><h2 id="卸载显卡驱动" tabindex="-1">卸载显卡驱动 <a class="header-anchor" href="#卸载显卡驱动" aria-label="Permalink to &quot;卸载显卡驱动&quot;">​</a></h2><ul><li>ddu <a href="https://www.wagnardsoft.com/" target="_blank" rel="noreferrer">https://www.wagnardsoft.com/</a></li><li><a href="https://www.wagnardsoft.com/forums/viewtopic.php?t=5192" target="_blank" rel="noreferrer">https://www.wagnardsoft.com/forums/viewtopic.php?t=5192</a></li></ul><h2 id="操作1——初始化git-lfs命令" tabindex="-1">操作1——初始化git-lfs命令 <a class="header-anchor" href="#操作1——初始化git-lfs命令" aria-label="Permalink to &quot;操作1——初始化git-lfs命令&quot;">​</a></h2><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash</span></span>
<span class="line"><span>sudo apt-get install git-lfs</span></span>
<span class="line"><span>git lfs install</span></span></code></pre></div><h2 id="操作2——下载github仓库" tabindex="-1">操作2——下载github仓库 <a class="header-anchor" href="#操作2——下载github仓库" aria-label="Permalink to &quot;操作2——下载github仓库&quot;">​</a></h2><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>// 下载加速---https://www.autodl.com/docs/network_turbo/</span></span>
<span class="line"><span>source /etc/network_turbo</span></span>
<span class="line"><span></span></span>
<span class="line"><span>// 下载</span></span>
<span class="line"><span>git clone https://github.com/hiyouga/LLaMA-Factory.git</span></span></code></pre></div><h2 id="操作3-下载模型比如deepseek-ai-deepseek-r1-distill-qwen-1-5b" tabindex="-1">操作3--下载模型比如deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B <a class="header-anchor" href="#操作3-下载模型比如deepseek-ai-deepseek-r1-distill-qwen-1-5b" aria-label="Permalink to &quot;操作3--下载模型比如deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B&quot;">​</a></h2><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>git clone https://www.modelscope.cn/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B.git</span></span></code></pre></div><h2 id="操作4-创建python运行环境" tabindex="-1">操作4--创建python运行环境 <a class="header-anchor" href="#操作4-创建python运行环境" aria-label="Permalink to &quot;操作4--创建python运行环境&quot;">​</a></h2><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>// 创建环境</span></span>
<span class="line"><span>conda create -n llama_factory python=3.10</span></span>
<span class="line"><span>conda activate llama_factory</span></span>
<span class="line"><span></span></span>
<span class="line"><span>// 安装依赖</span></span>
<span class="line"><span>cd LLaMA-Factory</span></span>
<span class="line"><span>pip install -e .[torch,metrics]</span></span>
<span class="line"><span></span></span>
<span class="line"><span>// 运行环境</span></span>
<span class="line"><span>export CUDA_VISIBLE_DEVICES=0</span></span>
<span class="line"><span>python src/webui.py</span></span>
<span class="line"><span></span></span>
<span class="line"><span>// 出现如下运行成功</span></span>
<span class="line"><span>Running on local URL:  http://0.0.0.0:7860</span></span>
<span class="line"><span></span></span>
<span class="line"><span>// 通过audodl SSH 隧道工具进行端口映射</span></span>
<span class="line"><span>ssh指令 复制</span></span>
<span class="line"><span>ssh密码 复制</span></span>
<span class="line"><span>代理到本地端口： 将autodl服务器上端口（sshport） 复制到这里</span></span>
<span class="line"><span>代理到远程端口： 暂时没使用，默认即可</span></span>
<span class="line"><span></span></span>
<span class="line"><span>// 然后浏览器打开地址</span></span>
<span class="line"><span>http://127.0.0.1:sshport</span></span>
<span class="line"><span></span></span>
<span class="line"><span>//删除当前文件夹下的文件和文件夹</span></span>
<span class="line"><span>rm -rf \\*</span></span></code></pre></div><h2 id="浏览器运行" tabindex="-1">浏览器运行 <a class="header-anchor" href="#浏览器运行" aria-label="Permalink to &quot;浏览器运行&quot;">​</a></h2><h2 id="新的gpu租用平台" tabindex="-1">新的GPU租用平台 <a class="header-anchor" href="#新的gpu租用平台" aria-label="Permalink to &quot;新的GPU租用平台&quot;">​</a></h2><ul><li><a href="https://cloud.luchentech.com/" target="_blank" rel="noreferrer">https://cloud.luchentech.com/</a></li></ul><h2 id="微调大模型留一个方案" tabindex="-1">微调大模型留一个方案 <a class="header-anchor" href="#微调大模型留一个方案" aria-label="Permalink to &quot;微调大模型留一个方案&quot;">​</a></h2><ul><li><a href="https://github.com/modelscope/ms-swift/blob/main/README_CN.md" target="_blank" rel="noreferrer">https://github.com/modelscope/ms-swift/blob/main/README_CN.md</a></li></ul>`,52)]))}const b=e(t,[["render",p]]);export{u as __pageData,b as default};
